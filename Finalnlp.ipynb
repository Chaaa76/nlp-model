{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtAtA897p1aH9WbBuKkok2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chaaa76/nlp-model/blob/main/Finalnlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydcR7c6u9i1T"
      },
      "outputs": [],
      "source": [
        "# %% [code]\n",
        "# Install required libraries/dependencies\n",
        "!pip install -q transformers sentence-transformers pandas scikit-learn numpy optuna rank_bm25 datasets\n",
        "!pip install -q --upgrade transformers\n",
        "!pip install -q rank_bm25\n",
        "!pip install -q --upgrade datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import json\n",
        "import logging\n",
        "import optuna\n",
        "from datetime import datetime\n",
        "from pathlib import Path\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import ndcg_score\n",
        "from rank_bm25 import BM25Okapi\n",
        "from typing import List, Dict, Tuple, Optional, Union\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder, losses, InputExample\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "import time\n",
        "import re\n",
        "\n",
        "# Enhanced logging configuration\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.StreamHandler(),\n",
        "        logging.FileHandler('ordinance_retrieval.log')\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(\"OrdinanceRetrieval\")\n",
        "\n",
        "class OrdinanceRetrievalSystem:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the retrieval system with default parameters\"\"\"\n",
        "        self.df = None\n",
        "        self.model = None\n",
        "        self.cross_encoder = None\n",
        "        self.embeddings = None\n",
        "        self.bm25 = None\n",
        "        self.tokenizer = None\n",
        "        self.id_to_idx = {}\n",
        "        self.available_ordinance_ids = set()\n",
        "        self.best_params = {\n",
        "            'semantic_weight': 0.6,\n",
        "            'ce_weight': 0.5,\n",
        "            'batch_size': 8,\n",
        "            'learning_rate': 3e-5,\n",
        "            'epochs': 2\n",
        "        }\n",
        "        self.query_expansion_terms = {\n",
        "        \"tax\": [\"levy\", \"revenue\", \"assessment\", \"dues\", \"collection\", \"fiscal\", \"fee\", \"amusement tax\", \"real property tax\"],\n",
        "        \"property\": [\"real estate\", \"land\", \"building\", \"house\", \"lot\", \"title\", \"structure\", \"premises\", \"estate\", \"realty\"],\n",
        "        \"business\": [\"enterprise\", \"commerce\", \"establishment\", \"shop\", \"store\", \"business permit\", \"commercial\", \"firm\", \"company\", \"trade\"],\n",
        "        \"health\": [\"sanitation\", \"hygiene\", \"medical\", \"clinic\", \"cleanliness\", \"public health\", \"healthcare\", \"infection control\", \"health program\"],\n",
        "        \"smoking\": [\"tobacco\", \"cigarette\", \"vape\", \"e-cigarette\", \"nicotine\", \"no smoking\", \"smoke-free\", \"secondhand smoke\", \"smoking ban\"],\n",
        "        \"alcohol\": [\"liquor\", \"alcoholic drink\", \"alcoholic beverage\", \"booze\", \"beer\", \"wine\", \"hard drinks\", \"intoxicating drink\", \"drinking ban\", \"liquor ban\", \"alcoholic\",\"drinking alcohol\",\"sell liquor to minors\"],\n",
        "        \"soft drinks\": [\"carbonated drink\", \"soda\", \"beverage\", \"bottled drink\", \"sweetened drink\", \"soft beverage\", \"colas\", \"refreshment\"],\n",
        "        \"license\": [\"permit\", \"authorization\", \"certification\", \"registration\", \"approval\", \"license to operate\", \"business license\", \"franchise\"],\n",
        "        \"waste\": [\"waste disposal\", \"garbage\", \"trash\", \"refuse\", \"rubbish\", \"disposal\", \"solid waste\", \"junk\", \"basura\", \"collection schedule\", \"segregation\"],\n",
        "        \"littering\": [\"waste disposal\", \"garbage\", \"trash\", \"illegal dumping\", \"rubbish\", \"improper disposal\", \"throwing waste\", \"scattering garbage\", \"basura\"],\n",
        "        \"cleanliness\": [\"sanitation\", \"cleaning\", \"public space\", \"tidy\", \"sweeping\", \"maintenance\", \"clean-up\", \"street cleaning\", \"city hygiene\"],\n",
        "        \"curfew\": [\"time restriction\", \"minor restriction\", \"kids outside\", \"night ban\", \"youth curfew\", \"curfew hours\", \"discipline hours\", \"prohibited time\"],\n",
        "        \"noise\": [\"loud sounds\", \"speakers\", \"karaoke\", \"disturbance\", \"amplifier\", \"sound pollution\", \"unnecessary noise\", \"noise control\"],\n",
        "        \"vehicles\": [\"cars\", \"motorcycles\", \"jeepney\", \"tricycle\", \"parking\", \"traffic\", \"automobile\", \"vehicle regulation\", \"transport\"],\n",
        "        \"parking\": [\"no parking\", \"parked car\", \"tow\", \"illegal parking\", \"street parking\", \"reserved parking\", \"paid parking\", \"parking violation\"],\n",
        "        \"market\": [\"public market\", \"vendor\", \"stall\", \"palengke\", \"selling area\", \"wet market\", \"dry goods\", \"marketplace\", \"tiangge\"],\n",
        "        \"environment\": [\"pollution\", \"air quality\", \"green\", \"climate\", \"clean air\", \"ecology\", \"waste management\", \"environmental protection\", \"carbon\"],\n",
        "        \"vending\": [\"street vendor\", \"hawker\", \"sidewalk selling\", \"illegal seller\", \"ambulant vendor\", \"vending stall\", \"peddling\", \"vendor regulation\"],\n",
        "        \"construction\": [\"building\", \"renovation\", \"development\", \"permit to build\", \"excavation\", \"infrastructure\", \"structural work\", \"building code\"],\n",
        "        \"zoning\": [\"land use\", \"residential area\", \"commercial zone\", \"reclassification\", \"urban planning\", \"zone ordinance\", \"rezone\", \"land designation\"],\n",
        "        \"penalty\": [\"fine\", \"punishment\", \"fee\", \"ticket\", \"sanction\", \"imprisonment\", \"penalized\", \"violation consequence\", \"offense fee\"],\n",
        "        \"school\": [\"education\", \"student\", \"learning\", \"public school\", \"academic\", \"elementary\", \"high school\", \"teacher\", \"educational institution\"],\n",
        "        \"barangay\": [\"local government\", \"neighborhood\", \"community office\", \"barangay hall\", \"barangay captain\", \"local unit\", \"barangay council\"],\n",
        "        \"senior citizen\": [\"elderly\", \"senior\", \"discount\", \"benefit\", \"ID\", \"senior card\", \"pension\", \"social protection\", \"elder care\",\"allowance\",\"monthly\",\"old people\",\"old\"],\n",
        "        \"PWD\": [\"disabled\", \"person with disability\", \"handicapped\", \"benefits\", \"ID\", \"PWD card\", \"accessibility\", \"inclusive\", \"disability support\"],\n",
        "        \"animal\": [\"dog\", \"cat\", \"pet\", \"stray\", \"animal control\", \"bite\", \"rabies\", \"pet registration\", \"veterinary\", \"animal welfare\"]\n",
        "        }\n",
        "\n",
        "\n",
        "    def load_data(self, file_path: str) -> pd.DataFrame:\n",
        "        \"\"\"Load and preprocess the ordinance data with robust error handling\"\"\"\n",
        "        try:\n",
        "            logger.info(f\"Loading data from {file_path}...\")\n",
        "\n",
        "            # Validate file existence\n",
        "            if not os.path.exists(file_path):\n",
        "                raise FileNotFoundError(f\"Data file not found at {file_path}\")\n",
        "\n",
        "            # Load with proper NA handling and dtype specification\n",
        "            dtype_mapping = {\n",
        "                'ordinance_id': str,\n",
        "                'short_text': str,\n",
        "                'full_text': str,\n",
        "                'category': str,\n",
        "                'fines': str,\n",
        "                'date_enacted': str,\n",
        "                'status': str,\n",
        "                'links': str\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                df = pd.read_csv(\n",
        "                    file_path,\n",
        "                    na_values=[\"nan\", \"NaN\", \"NULL\", \"None\", \"MISSING\", \"TOO LONG\", \"\"],\n",
        "                    dtype=dtype_mapping\n",
        "                )\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to read CSV: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "            # Validate required columns\n",
        "            required_columns = ['ordinance_id', 'short_text']\n",
        "            missing_cols = [col for col in required_columns if col not in df.columns]\n",
        "            if missing_cols:\n",
        "                raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
        "\n",
        "            # Text cleaning with error handling\n",
        "            text_cols = [\"short_text\", \"full_text\", \"category\", \"fines\",\"status\",\"links\"]\n",
        "            for col in text_cols:\n",
        "                if col in df.columns:\n",
        "                    try:\n",
        "                        df[col] = df[col].fillna(\"\").apply(\n",
        "                            lambda x: \" \".join(str(x).split()) if pd.notna(x) else \"\"\n",
        "                        )\n",
        "                    except Exception as e:\n",
        "                        logger.warning(f\"Error cleaning column {col}: {str(e)}\")\n",
        "                        df[col] = df[col].astype(str).fillna(\"\")\n",
        "\n",
        "            # Handle missing categories\n",
        "            if \"category\" in df.columns:\n",
        "                df[\"category\"] = df[\"category\"].replace(\"\", \"Unknown\").fillna(\"Unknown\")\n",
        "            # Handle missing status\n",
        "            if \"status\" in df.columns:\n",
        "                df[\"status\"] = df[\"status\"].replace(\"\", \"Status not specified\").fillna(\"Status not specified\")\n",
        "\n",
        "            # Standardize dates with error handling\n",
        "            if \"date_enacted\" in df.columns:\n",
        "                try:\n",
        "                    df[\"date_enacted\"] = pd.to_datetime(\n",
        "                        df[\"date_enacted\"],\n",
        "                        errors=\"coerce\",\n",
        "                        format='mixed'\n",
        "                    ).dt.strftime('%B %d, %Y')\n",
        "                    df[\"date_enacted\"] = df[\"date_enacted\"].fillna(\"Date not available\")\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error parsing dates: {str(e)}\")\n",
        "                    df[\"date_enacted\"] = \"Date not available\"\n",
        "\n",
        "            # Remove very short or empty texts\n",
        "            if \"short_text\" in df.columns:\n",
        "                df = df[df[\"short_text\"].str.len() > 30].copy()\n",
        "\n",
        "            self.df = df.reset_index(drop=True)\n",
        "            self.available_ordinance_ids = set(self.df[\"ordinance_id\"].astype(str).unique())\n",
        "            logger.info(f\"Successfully loaded {len(self.df)} ordinances\")\n",
        "            return self.df\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Critical error in load_data: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def initialize_models(self):\n",
        "        \"\"\"Initialize the retrieval models with error handling\"\"\"\n",
        "        try:\n",
        "            logger.info(\"Initializing models...\")\n",
        "\n",
        "            # Initialize tokenizer first\n",
        "            try:\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained('nlpaueb/legal-bert-base-uncased')\n",
        "                logger.info(\"Tokenizer initialized successfully\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Failed to load legal-bert tokenizer, falling back to default: {str(e)}\")\n",
        "                try:\n",
        "                    self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "                    logger.info(\"Fallback tokenizer initialized successfully\")\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Failed to initialize any tokenizer: {str(e)}\")\n",
        "                    self.tokenizer = None\n",
        "\n",
        "            # Bi-encoder for semantic search with fallback\n",
        "            try:\n",
        "                self.model = SentenceTransformer('nlpaueb/legal-bert-base-uncased')\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Failed to load legal-bert, falling back to all-MiniLM: {str(e)}\")\n",
        "                self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "            # Cross-encoder for re-ranking with fallback\n",
        "            try:\n",
        "                self.cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Failed to load cross-encoder: {str(e)}\")\n",
        "                self.cross_encoder = None\n",
        "\n",
        "            # BM25 for keyword search\n",
        "            try:\n",
        "                if self.df is not None and len(self.df) > 0 and self.tokenizer is not None:\n",
        "                    tokenized_corpus = [self.tokenizer.tokenize(str(text)) for text in self.df[\"short_text\"]]\n",
        "                    self.bm25 = BM25Okapi(tokenized_corpus)\n",
        "                    logger.info(\"BM25 initialized successfully\")\n",
        "                else:\n",
        "                    logger.warning(\"No data or tokenizer available for BM25 initialization\")\n",
        "                    self.bm25 = None\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Failed to initialize BM25: {str(e)}\")\n",
        "                self.bm25 = None\n",
        "\n",
        "            logger.info(\"Models initialized successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error initializing models: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def generate_embeddings(self):\n",
        "        \"\"\"Generate embeddings for all ordinances with error handling\"\"\"\n",
        "        try:\n",
        "            logger.info(\"Generating embeddings...\")\n",
        "\n",
        "            if self.model is None:\n",
        "                raise ValueError(\"Model not initialized\")\n",
        "\n",
        "            if len(self.df) == 0:\n",
        "                raise ValueError(\"No data available for embedding generation\")\n",
        "\n",
        "            # Use both short_text and category for better embeddings\n",
        "            texts = self.df[\"short_text\"].astype(str) + \" [SEP] \" + self.df[\"category\"].astype(str)\n",
        "\n",
        "            # Process in chunks to handle memory constraints\n",
        "            chunk_size = 100\n",
        "            embeddings = []\n",
        "            for i in range(0, len(texts), chunk_size):\n",
        "                chunk = texts[i:i + chunk_size].tolist()\n",
        "                try:\n",
        "                    embeddings.append(self.model.encode(chunk, show_progress_bar=False))\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error encoding chunk {i//chunk_size}: {str(e)}\")\n",
        "                    raise\n",
        "\n",
        "            self.embeddings = np.concatenate(embeddings)\n",
        "            self.id_to_idx = {str(id): idx for idx, id in enumerate(self.df[\"ordinance_id\"])}\n",
        "\n",
        "            logger.info(f\"Generated embeddings for {len(self.embeddings)} ordinances\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error generating embeddings: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def save_best_checkpoint(self, model_path: str = './best_model'):\n",
        "        \"\"\"Save the complete system state including tokenizer info\"\"\"\n",
        "        try:\n",
        "            os.makedirs(model_path, exist_ok=True)\n",
        "\n",
        "            # Save core models\n",
        "            self.model.save(model_path)\n",
        "\n",
        "            # Save tokenizer info\n",
        "            if self.tokenizer:\n",
        "                tokenizer_info = {\n",
        "                    'name_or_path': self.tokenizer.name_or_path,\n",
        "                    'special_tokens_map': self.tokenizer.special_tokens_map,\n",
        "                    'init_kwargs': self.tokenizer.init_kwargs\n",
        "                }\n",
        "                with open(os.path.join(model_path, 'tokenizer_config.json'), 'w') as f:\n",
        "                    json.dump(tokenizer_info, f, indent=4)\n",
        "                self.tokenizer.save_pretrained(model_path)\n",
        "\n",
        "            # Save data and metadata\n",
        "            if self.df is not None:\n",
        "                self.df.to_csv(os.path.join(model_path, 'data.csv'), index=False)\n",
        "\n",
        "            metadata = {\n",
        "                'best_params': self.best_params,\n",
        "                'best_checkpoint': self.best_checkpoint,\n",
        "                'retrieval_config': {\n",
        "                    'default_k': getattr(self, 'default_k', 1),\n",
        "                    'score_threshold': getattr(self, 'score_threshold', 50.0),\n",
        "                    'max_same_category': getattr(self, 'max_same_category', 2),\n",
        "                    'tokenizer_type': 'legal-bert' if 'legal-bert' in str(self.tokenizer) else 'bert-base'\n",
        "                },\n",
        "                'components': {\n",
        "                    'model_type': str(type(self.model)),\n",
        "                    'cross_encoder_type': str(type(self.cross_encoder)) if self.cross_encoder else None,\n",
        "                    'bm25_initialized': self.bm25 is not None\n",
        "                },\n",
        "                'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "            }\n",
        "\n",
        "            with open(os.path.join(model_path, 'metadata.json'), 'w') as f:\n",
        "                json.dump(metadata, f, indent=4)\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error saving complete checkpoint: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def load_best_checkpoint(self, model_path: str = './best_model'):\n",
        "        \"\"\"Load complete system state with tokenizer reconstruction\"\"\"\n",
        "        try:\n",
        "            if not os.path.exists(model_path):\n",
        "                raise FileNotFoundError(f\"Checkpoint directory not found at {model_path}\")\n",
        "\n",
        "            # Load metadata first\n",
        "            metadata_path = os.path.join(model_path, 'metadata.json')\n",
        "            if not os.path.exists(metadata_path):\n",
        "                raise FileNotFoundError(\"Missing metadata file in checkpoint\")\n",
        "\n",
        "            with open(metadata_path, 'r') as f:\n",
        "                metadata = json.load(f)\n",
        "                self.best_params = metadata.get('best_params', self.best_params)\n",
        "                self.best_checkpoint = metadata.get('best_checkpoint', None)\n",
        "\n",
        "                # Load retrieval config\n",
        "                retrieval_config = metadata.get('retrieval_config', {})\n",
        "                self.default_k = retrieval_config.get('default_k', 1)\n",
        "                self.score_threshold = retrieval_config.get('score_threshold', 50.0)\n",
        "                self.max_same_category = retrieval_config.get('max_same_category', 2)\n",
        "\n",
        "                # Verify component compatibility\n",
        "                components = metadata.get('components', {})\n",
        "                if 'legal-bert' not in components.get('tokenizer_type', ''):\n",
        "                    logger.warning(\"Original tokenizer was not legal-bert - performance may vary\")\n",
        "\n",
        "            # Load model\n",
        "            self.model = SentenceTransformer(model_path)\n",
        "\n",
        "            # Reinitialize tokenizer exactly as before\n",
        "            tokenizer_config_path = os.path.join(model_path, 'tokenizer_config.json')\n",
        "            if os.path.exists(tokenizer_config_path):\n",
        "                with open(tokenizer_config_path) as f:\n",
        "                    tokenizer_info = json.load(f)\n",
        "                try:\n",
        "                    self.tokenizer = AutoTokenizer.from_pretrained(\n",
        "                        tokenizer_info['name_or_path'],\n",
        "                        **tokenizer_info['init_kwargs']\n",
        "                    )\n",
        "                except:\n",
        "                    logger.warning(\"Failed to load original tokenizer, falling back to default\")\n",
        "                    self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "            else:\n",
        "                logger.warning(\"No tokenizer config found, initializing default tokenizer\")\n",
        "                self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "            # Load data\n",
        "            data_path = os.path.join(model_path, 'data.csv')\n",
        "            if os.path.exists(data_path):\n",
        "                self.load_data(data_path)\n",
        "            else:\n",
        "                logger.warning(\"No data file found in checkpoint\")\n",
        "\n",
        "            # Recreate BM25 index\n",
        "            if self.df is not None and len(self.df) > 0 and self.tokenizer:\n",
        "                tokenized_corpus = [self.tokenizer.tokenize(str(text)) for text in self.df[\"short_text\"]]\n",
        "                self.bm25 = BM25Okapi(tokenized_corpus)\n",
        "                logger.info(\"Recreated BM25 index from loaded data\")\n",
        "\n",
        "            # Regenerate embeddings\n",
        "            if self.df is not None and len(self.df) > 0:\n",
        "                self.generate_embeddings()\n",
        "                logger.info(\"Regenerated embeddings from loaded data\")\n",
        "\n",
        "            # Verify cross-encoder\n",
        "            if metadata.get('components', {}).get('cross_encoder_type'):\n",
        "                try:\n",
        "                    self.cross_encoder = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "                except:\n",
        "                    logger.warning(\"Failed to reload cross-encoder\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error loading complete checkpoint: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def verify_consistency(self):\n",
        "        \"\"\"Verify all components are properly initialized\"\"\"\n",
        "        checks = {\n",
        "            'model': isinstance(self.model, SentenceTransformer),\n",
        "            'tokenizer': hasattr(self.tokenizer, 'tokenize'),\n",
        "            'embeddings': self.embeddings is not None and len(self.embeddings) == len(self.df),\n",
        "            'bm25': self.bm25 is not None,\n",
        "            'cross_encoder': self.cross_encoder is None or isinstance(self.cross_encoder, CrossEncoder),\n",
        "            'parameters': all(k in self.best_params for k in ['semantic_weight', 'ce_weight'])\n",
        "        }\n",
        "\n",
        "        if not all(checks.values()):\n",
        "            logger.error(f\"Consistency check failed: {checks}\")\n",
        "            return False\n",
        "\n",
        "        logger.info(\"All components verified and consistent\")\n",
        "        return True\n",
        "\n",
        "\n",
        "    def train_retrieval_model(self, epochs: int = None, batch_size: int = None,\n",
        "                            learning_rate: float = None) -> None:\n",
        "        \"\"\"Train the retrieval model with contrastive learning and error handling\"\"\"\n",
        "        try:\n",
        "            logger.info(\"Training retrieval model...\")\n",
        "\n",
        "            # Use best params if none provided\n",
        "            epochs = epochs or self.best_params['epochs']\n",
        "            batch_size = batch_size or self.best_params['batch_size']\n",
        "            learning_rate = learning_rate or self.best_params['learning_rate']\n",
        "\n",
        "            # Reduce batch size to alleviate memory pressure\n",
        "            batch_size = min(batch_size, 8)\n",
        "\n",
        "            # Prepare training examples\n",
        "            train_examples = []\n",
        "            for _, row in self.df.iterrows():\n",
        "                try:\n",
        "                    # Positive example (same category)\n",
        "                    same_cat = self.df[self.df[\"category\"] == row[\"category\"]].sample(1)\n",
        "                    if len(same_cat) > 0:\n",
        "                        train_examples.append(InputExample(\n",
        "                            texts=[str(row[\"short_text\"]), str(same_cat.iloc[0][\"short_text\"])],\n",
        "                            label=1.0))\n",
        "\n",
        "                    # Negative example (different category)\n",
        "                    diff_cat = self.df[self.df[\"category\"] != row[\"category\"]].sample(1)\n",
        "                    if len(diff_cat) > 0:\n",
        "                        train_examples.append(InputExample(\n",
        "                            texts=[str(row[\"short_text\"]), str(diff_cat.iloc[0][\"short_text\"])],\n",
        "                            label=0.0))\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error creating training example for row {row.name}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            if not train_examples:\n",
        "                raise ValueError(\"No valid training examples could be created\")\n",
        "\n",
        "            # Create dataloader\n",
        "            try:\n",
        "                train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)\n",
        "                train_loss = losses.CosineSimilarityLoss(self.model)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error creating dataloader: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "            # Configure training\n",
        "            warmup_steps = min(100, len(train_dataloader) * epochs // 10)\n",
        "\n",
        "            # Train the model with progress tracking\n",
        "            try:\n",
        "                self.model.fit(\n",
        "                    train_objectives=[(train_dataloader, train_loss)],\n",
        "                    epochs=epochs,\n",
        "                    warmup_steps=warmup_steps,\n",
        "                    optimizer_params={'lr': learning_rate},\n",
        "                    show_progress_bar=True\n",
        "                )\n",
        "\n",
        "                # Save the final checkpoint info\n",
        "                self.best_checkpoint = {\n",
        "                    'epochs': epochs,\n",
        "                    'batch_size': batch_size,\n",
        "                    'learning_rate': learning_rate,\n",
        "                    'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
        "                }\n",
        "\n",
        "                # After training completes:\n",
        "                retrieval_system.default_k = 1  # Set your desired default\n",
        "                retrieval_system.score_threshold = 70.0  # Higher threshold for single result\n",
        "                retrieval_system.max_same_category = 1  # Strict category limiting\n",
        "\n",
        "                # Verify before saving\n",
        "                if not retrieval_system.verify_consistency():\n",
        "                    raise RuntimeError(\"System not in consistent state for saving\")\n",
        "\n",
        "                retrieval_system.save_best_checkpoint()\n",
        "\n",
        "                logger.info(\"Model training completed successfully\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error during model training: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in train_retrieval_model: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "    def expand_query(self, query: str) -> str:\n",
        "        \"\"\"Enhanced query expansion that better integrates synonyms\"\"\"\n",
        "        try:\n",
        "            expanded = query.lower()\n",
        "            tokens = set(re.findall(r\"\\w+\", expanded))\n",
        "\n",
        "            # Add synonyms for each token found\n",
        "            for term, synonyms in self.query_expansion_terms.items():\n",
        "                if term in tokens:\n",
        "                    expanded += \" \" + \" \".join(synonyms[:5])\n",
        "\n",
        "            # Also check for multi-word terms\n",
        "            for term, synonyms in self.query_expansion_terms.items():\n",
        "                if ' ' in term and term in expanded:\n",
        "                    expanded += \" \" + \" \".join(synonyms[:3])\n",
        "\n",
        "            return \" \".join(list(set(expanded.split())))\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Query expansion failed: {str(e)}\")\n",
        "            return query\n",
        "\n",
        "    def _is_ordinance_id_query(self, query: str) -> bool:\n",
        "        \"\"\"Check if the query is specifically looking for an ordinance ID\"\"\"\n",
        "        # Patterns like \"Ordinance 1234\" or just \"1234\"\n",
        "        return bool(re.match(r'(ordinance\\s*)?\\d+', query.lower()))\n",
        "\n",
        "    def _get_ordinance_by_id(self, ordinance_id: str) -> Optional[Dict]:\n",
        "        \"\"\"Retrieve a single ordinance by ID if it exists\"\"\"\n",
        "        try:\n",
        "            # Clean the ID string\n",
        "            ordinance_id = re.sub(r'[^0-9]', '', ordinance_id)\n",
        "            if not ordinance_id:\n",
        "                return None\n",
        "\n",
        "            # Check if ID exists\n",
        "            if ordinance_id not in self.available_ordinance_ids:\n",
        "                return {\n",
        "                    \"ordinance_id\": ordinance_id,\n",
        "                    \"category\": \"N/A\",\n",
        "                    \"short_text\": f\"Ordinance {ordinance_id} data entry is MISSING from our records.\",\n",
        "                    \"fines\": \"N/A\",\n",
        "                    \"date_enacted\": \"N/A\",\n",
        "                    \"status\": \"N/A\",\n",
        "                    \"links\": \"N/A\",\n",
        "                    \"confidence\": \"Not Found\",\n",
        "                    \"score\": \"0.0%\"\n",
        "                }\n",
        "\n",
        "            # Get the ordinance data\n",
        "            idx = self.id_to_idx.get(ordinance_id)\n",
        "            if idx is None or idx >= len(self.df):\n",
        "                return {\n",
        "                    \"ordinance_id\": ordinance_id,\n",
        "                    \"category\": \"N/A\",\n",
        "                    \"short_text\": f\"Ordinance {ordinance_id} data entry is MISSING from our records.\",\n",
        "                    \"fines\": \"N/A\",\n",
        "                    \"date_enacted\": \"N/A\",\n",
        "                    \"status\": \"N/A\",\n",
        "                    \"links\": \"N/A\",\n",
        "                    \"confidence\": \"Not Found\",\n",
        "                    \"score\": \"0.0%\"\n",
        "                }\n",
        "\n",
        "            row = self.df.iloc[idx]\n",
        "            return {\n",
        "                \"ordinance_id\": row[\"ordinance_id\"],\n",
        "                \"category\": row.get(\"category\", \"\"),\n",
        "                \"short_text\": row[\"short_text\"],\n",
        "                \"fines\": row.get(\"fines\", \"\"),\n",
        "                \"date_enacted\": row.get(\"date_enacted\", \"\"),\n",
        "                \"status\": row.get(\"status\", \"Status not specified\"),\n",
        "                \"links\": row.get(\"links\", \"Link not available\"),\n",
        "                \"confidence\": \"High\",\n",
        "                \"score\": \"100.0%\"\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error retrieving ordinance by ID {ordinance_id}: {str(e)}\")\n",
        "            return {\n",
        "                \"ordinance_id\": ordinance_id,\n",
        "                \"category\": \"N/A\",\n",
        "                \"short_text\": f\"Ordinance {ordinance_id} data entry is MISSING from our records.\",\n",
        "                \"fines\": \"N/A\",\n",
        "                \"date_enacted\": \"N/A\",\n",
        "                \"status\": \"N/A\",\n",
        "                \"links\": \"N/A\",\n",
        "                \"confidence\": \"Error\",\n",
        "                \"score\": \"0.0%\"\n",
        "            }\n",
        "\n",
        "    def robust_scale(self, arr: np.ndarray) -> np.ndarray:\n",
        "        \"\"\"More robust scaling that handles edge cases\"\"\"\n",
        "        try:\n",
        "            arr = np.array(arr)\n",
        "            if np.all(arr == arr[0]):  # All values equal\n",
        "                return np.ones_like(arr) * 0.5  # Return neutral score\n",
        "            return (arr - np.min(arr)) / (np.ptp(arr) + 1e-6)  # Add small epsilon\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error in robust_scale: {str(e)}\")\n",
        "            return np.zeros_like(arr)\n",
        "\n",
        "    def calculate_accuracy_score(self, query_emb: np.ndarray, doc_emb: np.ndarray,\n",
        "                               semantic_score: float, keyword_score: float,\n",
        "                               ce_score: Optional[float] = None) -> float:\n",
        "        \"\"\"Calculate standardized accuracy score for a match\"\"\"\n",
        "        try:\n",
        "            # Calculate cosine similarity between query and document embeddings\n",
        "            similarity = cosine_similarity([query_emb], [doc_emb])[0][0]\n",
        "\n",
        "            # Normalize scores to [0, 1] range\n",
        "            semantic_norm = (semantic_score + 1) / 2  # Convert from [-1, 1] to [0, 1]\n",
        "            keyword_norm = keyword_score / np.max(keyword_score) if np.max(keyword_score) > 0 else 0\n",
        "\n",
        "            # Combine scores with weights\n",
        "            if ce_score is not None:\n",
        "                ce_norm = (ce_score + 1) / 2  # Convert from [-1, 1] to [0, 1]\n",
        "                combined_score = (\n",
        "                    0.4 * semantic_norm +  # Semantic similarity weight\n",
        "                    0.3 * keyword_norm +   # Keyword match weight\n",
        "                    0.3 * ce_norm          # Cross-encoder weight\n",
        "                )\n",
        "            else:\n",
        "                combined_score = (\n",
        "                    0.6 * semantic_norm +  # Higher weight for semantic when no CE\n",
        "                    0.4 * keyword_norm     # Lower weight for keyword when no CE\n",
        "                )\n",
        "\n",
        "            # Convert to percentage (0-100%) and cap at 100%\n",
        "            accuracy_score = min(combined_score * 100, 100.0)\n",
        "\n",
        "            return accuracy_score\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Error calculating accuracy score: {str(e)}\")\n",
        "            return 0.0\n",
        "\n",
        "    def _get_confidence_level(self, score: float) -> str:\n",
        "        \"\"\"Convert numeric score to confidence level\"\"\"\n",
        "        try:\n",
        "            score = float(score)\n",
        "            if score > 70:  # 70%\n",
        "                return \"High\"\n",
        "            elif score > 50:  # 50%\n",
        "                return \"Medium\"\n",
        "            elif score > 30:  # 30%\n",
        "                return \"Low\"\n",
        "            return \"Very Low\"\n",
        "        except:\n",
        "            return \"Unknown\"\n",
        "\n",
        "    def retrieve_ordinances(self, query: str, k: int = 5,\n",
        "                      semantic_weight: float = None,\n",
        "                      ce_weight: float = None) -> Union[List[Dict], str]:\n",
        "        try:\n",
        "            # Check for ordinance ID query first\n",
        "            if self._is_ordinance_id_query(query):\n",
        "                ordinance_id = re.sub(r'[^0-9]', '', query)\n",
        "                ordinance = self._get_ordinance_by_id(ordinance_id)\n",
        "                if ordinance:\n",
        "                    return [ordinance]  # Return as a list to maintain consistent return type\n",
        "                else:\n",
        "                    return [{\n",
        "                        \"ordinance_id\": ordinance_id,\n",
        "                        \"category\": \"N/A\",\n",
        "                        \"short_text\": f\"Ordinance {ordinance_id} data entry is MISSING from our records.\",\n",
        "                        \"fines\": \"N/A\",\n",
        "                        \"date_enacted\": \"N/A\",\n",
        "                        \"status\": \"N/A\",\n",
        "                        \"links\": \"N/A\",\n",
        "                        \"confidence\": \"Not Found\",\n",
        "                        \"score\": \"0.0%\"\n",
        "                    }]\n",
        "\n",
        "            # Use best params if none provided\n",
        "            semantic_weight = semantic_weight or self.best_params['semantic_weight']\n",
        "            ce_weight = ce_weight or self.best_params['ce_weight']\n",
        "            final_ce_weight = ce_weight\n",
        "            final_semantic_weight = 1 - ce_weight\n",
        "\n",
        "            # Enhanced query expansion with better integration\n",
        "            try:\n",
        "                expanded_query = self.expand_query(query)\n",
        "                query_embs = self.model.encode([query, expanded_query], convert_to_tensor=True)\n",
        "                query_emb = torch.mean(query_embs, dim=0).cpu().numpy()\n",
        "\n",
        "                # Enhanced tokenization for BM25 that includes expanded terms\n",
        "                tokenized_query = self.tokenizer.tokenize(query.lower())\n",
        "                expanded_tokens = self.tokenizer.tokenize(expanded_query.lower())\n",
        "                all_tokens = tokenized_query * 2 + expanded_tokens  # Give original terms 2x weight\n",
        "\n",
        "                logger.info(f\"Original query: {query}\")\n",
        "                logger.info(f\"Expanded query: {expanded_query}\")\n",
        "                logger.info(f\"All search tokens: {all_tokens}\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Query expansion failed, using original query: {str(e)}\")\n",
        "                query_emb = self.model.encode(query, convert_to_tensor=True).cpu().numpy()\n",
        "                all_tokens = self.tokenizer.tokenize(query.lower())\n",
        "            # Step 2: First-stage retrieval with enhanced scoring\n",
        "            # Semantic similarity with chunk processing for large datasets\n",
        "            semantic_scores = []\n",
        "            chunk_size = 500  # Process embeddings in chunks to avoid memory issues\n",
        "            for i in range(0, len(self.embeddings), chunk_size):\n",
        "                try:\n",
        "                    chunk = self.embeddings[i:i + chunk_size]\n",
        "                    scores = cosine_similarity([query_emb], chunk)[0]\n",
        "                    semantic_scores.extend(scores)\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error processing chunk {i//chunk_size}: {str(e)}\")\n",
        "                    semantic_scores.extend([0] * len(chunk))\n",
        "\n",
        "            semantic_scores = np.array(semantic_scores)\n",
        "            semantic_scores_norm = self.robust_scale(semantic_scores)\n",
        "\n",
        "            # Enhanced keyword search with multiple fields\n",
        "            keyword_scores = np.zeros(len(semantic_scores_norm))\n",
        "            if self.tokenizer is not None and self.bm25 is not None:\n",
        "                try:\n",
        "                    # Use the combined tokens (original + expanded) for BM25\n",
        "                    text_scores = self.bm25.get_scores(all_tokens)\n",
        "\n",
        "                    # If available, also search in category field\n",
        "                    if 'category' in self.df.columns:\n",
        "                        category_bm25 = BM25Okapi([self.tokenizer.tokenize(str(cat).lower())\n",
        "                                                for cat in self.df['category']])\n",
        "                        category_scores = category_bm25.get_scores(all_tokens)\n",
        "                        keyword_scores = 0.7 * text_scores + 0.3 * category_scores\n",
        "                    else:\n",
        "                        keyword_scores = text_scores\n",
        "\n",
        "                    keyword_scores_norm = self.robust_scale(keyword_scores)\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Keyword search failed: {str(e)}\")\n",
        "                    keyword_scores_norm = np.zeros(len(semantic_scores_norm))\n",
        "            else:\n",
        "                logger.warning(\"Keyword search not available (tokenizer or BM25 not initialized)\")\n",
        "                keyword_scores_norm = np.zeros(len(semantic_scores_norm))\n",
        "\n",
        "            # Get top candidates with diversity\n",
        "            try:\n",
        "                top_indices = np.argsort(semantic_scores)[::-1][:200]  # Wider initial pool\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error sorting scores: {str(e)}\")\n",
        "                top_indices = np.arange(len(semantic_scores))[:200]\n",
        "\n",
        "            # Step 3: Enhanced re-ranking with cross-encoder if available\n",
        "            ce_scores = None\n",
        "            if self.cross_encoder:\n",
        "                try:\n",
        "                    # Prepare richer context for cross-encoder using expanded query\n",
        "                    pairs = []\n",
        "                    for idx in top_indices:\n",
        "                        row = self.df.iloc[idx]\n",
        "                        context = f\"Category: {row.get('category', '')}. \"\n",
        "                        context += f\"Text: {row['short_text']}. \"\n",
        "                        if pd.notna(row.get('fines', None)):\n",
        "                            context += f\"Fines: {row['fines']}. \"\n",
        "                        if pd.notna(row.get('date_enacted', None)):\n",
        "                            context += f\"Enacted: {row['date_enacted']}.\"\n",
        "                        pairs.append((expanded_query, context))  # Use expanded query here\n",
        "\n",
        "                    ce_scores = self.cross_encoder.predict(pairs, show_progress_bar=False)\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Cross-encoder failed: {str(e)}\")\n",
        "\n",
        "            # Calculate accuracy scores for all candidates\n",
        "            accuracy_scores = []\n",
        "            for i, idx in enumerate(top_indices):\n",
        "                try:\n",
        "                    doc_emb = self.embeddings[idx]\n",
        "                    ce_score = ce_scores[i] if ce_scores is not None else None\n",
        "                    accuracy = self.calculate_accuracy_score(\n",
        "                        query_emb, doc_emb,\n",
        "                        semantic_scores[idx],\n",
        "                        keyword_scores[idx] if 'keyword_scores' in locals() else 0,\n",
        "                        ce_score\n",
        "                    )\n",
        "                    accuracy_scores.append((idx, accuracy))\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error calculating accuracy for index {idx}: {str(e)}\")\n",
        "                    accuracy_scores.append((idx, 0.0))\n",
        "\n",
        "            # Sort by accuracy score\n",
        "            accuracy_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Final ranking with diversity promotion and score threshold\n",
        "            final_indices = []\n",
        "            seen_categories = set()\n",
        "            score_threshold = 30.0  # Minimum accuracy score threshold (50%), now 30 for less lenient\n",
        "\n",
        "            for idx, score in accuracy_scores:\n",
        "                if len(final_indices) >= k:\n",
        "                    break\n",
        "\n",
        "                # Skip results below threshold\n",
        "                if score < score_threshold:\n",
        "                    continue\n",
        "\n",
        "                row = self.df.iloc[idx]\n",
        "                category = row.get('category', '')\n",
        "\n",
        "                # Promote diversity by limiting same-category results\n",
        "                if category not in seen_categories or len(seen_categories) >= 5:\n",
        "                    final_indices.append((idx, score))\n",
        "                    seen_categories.add(category)\n",
        "\n",
        "            # If no results meet the threshold, return a friendly message\n",
        "            if not final_indices:\n",
        "                return [{\n",
        "                    \"ordinance_id\": \"N/A\",\n",
        "                    \"category\": \"N/A\",\n",
        "                    \"short_text\": \"Sorry, I couldn't find any ordinance related to your question. Please try re-phrasing or adding more keywords.\",\n",
        "                    \"fines\": \"N/A\",\n",
        "                    \"date_enacted\": \"N/A\",\n",
        "                    \"status\": \"N/A\",\n",
        "                    \"links\": \"N/A\",\n",
        "                    \"confidence\": \"No Match\",\n",
        "                    \"score\": \"0.0%\"\n",
        "                }]\n",
        "\n",
        "            # Prepare detailed results\n",
        "            results = []\n",
        "            for idx, accuracy_score in final_indices:\n",
        "                try:\n",
        "                    row = self.df.iloc[idx]\n",
        "                    result = {\n",
        "                        \"ordinance_id\": row[\"ordinance_id\"],\n",
        "                        \"category\": row.get(\"category\", \"\"),\n",
        "                        \"short_text\": row[\"short_text\"],\n",
        "                        \"fines\": row.get(\"fines\", \"\"),\n",
        "                        \"date_enacted\": row.get(\"date_enacted\", \"\"),\n",
        "                        \"status\": row.get(\"status\", \"Status not specified\"),\n",
        "                        \"links\": row.get(\"links\", \"Links not specified\"),\n",
        "                        \"confidence\": self._get_confidence_level(accuracy_score / 100),  # Convert back to [0,1] range\n",
        "                        \"score\": f\"{accuracy_score:.1f}%\",\n",
        "                        \"details\": {\n",
        "                            \"accuracy_score\": f\"{accuracy_score:.1f}%\",\n",
        "                            \"semantic_score\": f\"{semantic_scores[idx]:.3f}\",\n",
        "                            \"keyword_score\": f\"{keyword_scores[idx]:.1f}\" if 'keyword_scores' in locals() else \"N/A\",\n",
        "                            \"cross_encoder_score\": f\"{ce_scores[i]:.1f}\" if ce_scores is not None else \"N/A\"\n",
        "                        }\n",
        "                    }\n",
        "                    results.append(result)\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error formatting result {idx}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            return results if results else [{\n",
        "                \"ordinance_id\": \"N/A\",\n",
        "                \"category\": \"N/A\",\n",
        "                \"short_text\": \"Sorry, I couldn't find any ordinance related to your question. Please try re-phrasing or adding more keywords\",\n",
        "                \"fines\": \"N/A\",\n",
        "                \"date_enacted\": \"N/A\",\n",
        "                \"status\": \"N/A\",\n",
        "                \"confidence\": \"No Match\",\n",
        "                \"score\": \"0.0%\"\n",
        "            }]\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in retrieve_ordinances: {str(e)}\")\n",
        "            return [{\n",
        "                \"ordinance_id\": \"N/A\",\n",
        "                \"category\": \"N/A\",\n",
        "                \"short_text\": \"Sorry, I couldn't find any ordinance related to your question. Please try re-phrasing or adding more keywords\",\n",
        "                \"fines\": \"N/A\",\n",
        "                \"date_enacted\": \"N/A\",\n",
        "                \"status\": \"N/A\",\n",
        "                \"links\": \"N/A\",\n",
        "                \"confidence\": \"Error\",\n",
        "                \"score\": \"0.0%\"\n",
        "            }]\n",
        "\n",
        "    def evaluate(self, test_queries: Dict[str, List[str]],\n",
        "                semantic_weight: float, ce_weight: float) -> float:\n",
        "        \"\"\"Evaluate retrieval performance using nDCG with error handling\"\"\"\n",
        "        try:\n",
        "            all_ndcg = []\n",
        "            for query, relevant_ids in test_queries.items():\n",
        "                try:\n",
        "                    results = self.retrieve_ordinances(\n",
        "                        query,\n",
        "                        k=1,\n",
        "                        semantic_weight=semantic_weight,\n",
        "                        ce_weight=ce_weight\n",
        "                    )\n",
        "\n",
        "                    # Handle case where retrieve_ordinances returns an error message\n",
        "                    if isinstance(results, str):\n",
        "                        logger.warning(f\"Evaluation failed for query '{query}': {results}\")\n",
        "                        continue\n",
        "\n",
        "                    retrieved_ids = [res[\"ordinance_id\"] for res in results]\n",
        "\n",
        "                    # Create relevance scores (1 for relevant, 0 otherwise)\n",
        "                    true_relevance = [1 if id in relevant_ids else 0 for id in retrieved_ids]\n",
        "\n",
        "                    # Skip if no relevant documents found\n",
        "                    if sum(true_relevance) == 0:\n",
        "                        continue\n",
        "\n",
        "                    ideal_relevance = sorted(true_relevance, reverse=True)\n",
        "\n",
        "                    # Calculate NDCG only if we have more than one document\n",
        "                    if len(true_relevance) > 1:\n",
        "                        ndcg = ndcg_score([true_relevance], [ideal_relevance])\n",
        "                        all_ndcg.append(ndcg)\n",
        "                except Exception as e:\n",
        "                    logger.warning(f\"Error evaluating query '{query}': {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            return np.mean(all_ndcg) if all_ndcg else 0.0\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error in evaluate: {str(e)}\")\n",
        "            return 0.0\n",
        "\n",
        "    def optimize_hyperparameters(self, test_queries: Dict[str, List[str]],\n",
        "                               n_trials: int = 20) -> None:\n",
        "            \"\"\"Optimize hyperparameters using Optuna with enhanced error handling\"\"\"\n",
        "            def objective(trial):\n",
        "                try:\n",
        "                    # Suggest hyperparameters\n",
        "                    params = {\n",
        "                        'semantic_weight': trial.suggest_float('semantic_weight', 0.4, 0.9),\n",
        "                        'ce_weight': trial.suggest_float('ce_weight', 0.4, 0.9),\n",
        "                        'batch_size': trial.suggest_categorical('batch_size', [8, 16, 32]),\n",
        "                        'learning_rate': trial.suggest_float('learning_rate', 1e-6, 5e-5, log=True),\n",
        "                        'epochs': trial.suggest_int('epochs', 1, 5)\n",
        "                    }\n",
        "\n",
        "                    # Train with these parameters\n",
        "                    self.train_retrieval_model(\n",
        "                        epochs=params['epochs'],\n",
        "                        batch_size=params['batch_size'],\n",
        "                        learning_rate=params['learning_rate']\n",
        "                    )\n",
        "\n",
        "                    # Generate new embeddings after training\n",
        "                    self.generate_embeddings()\n",
        "\n",
        "                    # Evaluate performance\n",
        "                    ndcg = self.evaluate(\n",
        "                        test_queries,\n",
        "                        semantic_weight=params['semantic_weight'],\n",
        "                        ce_weight=params['ce_weight']\n",
        "                    )\n",
        "\n",
        "                    return ndcg\n",
        "\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Trial failed: {str(e)}\")\n",
        "                    return 0.0  # Return minimum score for failed trials\n",
        "\n",
        "            try:\n",
        "                # Run optimization\n",
        "                study = optuna.create_study(direction='maximize')\n",
        "                study.optimize(objective, n_trials=n_trials)\n",
        "\n",
        "                # Store best parameters\n",
        "                self.best_params.update(study.best_params)\n",
        "                logger.info(f\"Best hyperparameters: {self.best_params}\")\n",
        "                logger.info(f\"Best nDCG score: {study.best_value:.4f}\")\n",
        "\n",
        "                # Train final model with best parameters\n",
        "                self.train_retrieval_model(\n",
        "                    epochs=self.best_params['epochs'],\n",
        "                    batch_size=self.best_params['batch_size'],\n",
        "                    learning_rate=self.best_params['learning_rate']\n",
        "                )\n",
        "                self.generate_embeddings()\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Hyperparameter optimization failed: {str(e)}\")\n",
        "                raise\n",
        "\n",
        "    def interactive_query_loop(self):\n",
        "        \"\"\"Run an interactive query loop with enhanced user experience\"\"\"\n",
        "        print(\"\\n=== Manila City Ordinance Retrieval System ===\")\n",
        "\n",
        "        # Print checkpoint information\n",
        "        if hasattr(self, 'best_checkpoint'):\n",
        "            print(\"Welcome!\")\n",
        "\n",
        "        else:\n",
        "            print(\"\\n Using default model parameters (no optimized checkpoint found)\")\n",
        "\n",
        "        print(\"\\nType 'exit' to quit the program\\n\")\n",
        "\n",
        "        while True:\n",
        "            try:\n",
        "                query = input(\"\\n Enter your query (or 'exit' to quit): \").strip()\n",
        "                if query.lower() == 'exit':\n",
        "                    break\n",
        "\n",
        "                if not query:\n",
        "                    print(\"Please enter a valid query\")\n",
        "                    continue\n",
        "\n",
        "                start_time = time.time()\n",
        "                results = self.retrieve_ordinances(query)\n",
        "                elapsed = time.time() - start_time\n",
        "\n",
        "                # Handle case where retrieve_ordinances returns an error message\n",
        "                if isinstance(results, str):\n",
        "                    print(f\"\\n{results}\")\n",
        "                    continue\n",
        "\n",
        "\n",
        "\n",
        "                for i, res in enumerate(results, 1):\n",
        "                    print(f\"#{i} - ID: {res['ordinance_id']} | Category: {res['category']} | Score: {res['score']}\")\n",
        "                    print(f\" Summary: {res['short_text']}\")\n",
        "                    print(f\" Fines: {res['fines']}\")\n",
        "                    print(f\" Status: {res['status']}\\n\")\n",
        "                    print(f\" Full Text/PDF: {res['links']}\")\n",
        "                    print(f\" Date Enacted: {res['date_enacted']}\\n\")\n",
        "\n",
        "                if results:  # Only show if there were any results\n",
        "                  print(\"\\n Jayoma Bot is an ordinance retrieval system designed to help users provide clarity to local laws in layman terms\")\n",
        "                  print(\"and should not be considered a substitute for professional legal advice or official legal interpretation.\\n\")\n",
        "\n",
        "            except KeyboardInterrupt:\n",
        "                print(\"\\nOperation cancelled by user\")\n",
        "                break\n",
        "            except Exception as e:\n",
        "                print(f\"\\nAn error occurred: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    def upload_dataset_colab(self) -> str:\n",
        "        \"\"\"Handle file upload in Google Colab environment\"\"\"\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            uploaded = files.upload()\n",
        "            if not uploaded:\n",
        "                raise ValueError(\"No file was uploaded\")\n",
        "            filename = list(uploaded.keys())[0]\n",
        "            logger.info(f\"Successfully uploaded file: {filename}\")\n",
        "            return filename\n",
        "        except ImportError:\n",
        "            logger.error(\"Google Colab module not found. This function only works in Google Colab environment.\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error uploading file: {str(e)}\")\n",
        "            raise\n",
        "\n",
        "# Example usage with comprehensive error handling\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Initialize system\n",
        "        retrieval_system = OrdinanceRetrievalSystem()\n",
        "\n",
        "        # ====== BEGIN UPDATED WORKFLOW ======\n",
        "        # First, check if valid checkpoint exists\n",
        "        checkpoint_path = './best_model'\n",
        "        checkpoint_valid = os.path.exists(checkpoint_path)\n",
        "\n",
        "        if checkpoint_valid:\n",
        "            try:\n",
        "                print(\" Loading existing optimized model...\")\n",
        "                retrieval_system.load_best_checkpoint()\n",
        "                print(\" Successfully loaded trained model\")\n",
        "            except Exception as e:\n",
        "                print(f\" Failed to load checkpoint (will train new model): {str(e)}\")\n",
        "                checkpoint_valid = False\n",
        "\n",
        "        if not checkpoint_valid:\n",
        "            print(\"\\n Initializing new model training workflow\")\n",
        "\n",
        "            # Load data first\n",
        "            data_file = None\n",
        "            try:\n",
        "                # Check if running in Google Colab\n",
        "                try:\n",
        "                    import google.colab\n",
        "                    is_colab = True\n",
        "                except ImportError:\n",
        "                    is_colab = False\n",
        "\n",
        "                # Load data based on environment\n",
        "                if is_colab:\n",
        "                    try:\n",
        "                        data_file = retrieval_system.upload_dataset_colab()\n",
        "                    except Exception as e:\n",
        "                        print(f\"Failed to upload file in Colab: {str(e)}\")\n",
        "                        exit(1)\n",
        "                else:\n",
        "                    data_file = \"ordinance_data.csv\"  # Local file\n",
        "                    if not os.path.exists(data_file):\n",
        "                        print(f\"Error: Data file '{data_file}' not found.\")\n",
        "                        print(\"Please ensure your CSV file is in the same directory as this script.\")\n",
        "                        exit(1)\n",
        "\n",
        "                # Load the data\n",
        "                print(\"\\nLoading data...\")\n",
        "                df = retrieval_system.load_data(data_file)\n",
        "                if df is None or len(df) == 0:\n",
        "                    raise ValueError(\"No data loaded or empty dataset\")\n",
        "                print(f\"Successfully loaded {len(df)} ordinances\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to load data: {str(e)}\")\n",
        "                exit(1)\n",
        "\n",
        "            # Initialize models after data is loaded\n",
        "            try:\n",
        "                print(\"\\nInitializing models...\")\n",
        "                retrieval_system.initialize_models()\n",
        "                print(\"Models initialized successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to initialize models: {str(e)}\")\n",
        "                exit(1)\n",
        "\n",
        "            # Generate embeddings\n",
        "            try:\n",
        "                print(\"\\nGenerating embeddings...\")\n",
        "                retrieval_system.generate_embeddings()\n",
        "                print(\"Embeddings generated successfully\")\n",
        "            except Exception as e:\n",
        "                print(f\"Failed to generate embeddings: {str(e)}\")\n",
        "                exit(1)\n",
        "\n",
        "            # Prepare test queries for optimization\n",
        "            test_queries = {\n",
        "                \"smoking ban\": [\"8521\", \"8677\", \"8563\", \"8521\"],\n",
        "                \"property tax\": [\"8516\", \"8503\", \"8467\", \"8461\", \"8454\"],\n",
        "                \"business license\": [\"8814\", \"8760\", \"8740\"],\n",
        "                \"public health\": [\"8800\", \"8797\", \"8781\", \"8779\"],\n",
        "                \"construction permit\": [\"8767\", \"8753\", \"8738\", \"8727\"]\n",
        "            }\n",
        "\n",
        "            # Run hyperparameter optimization\n",
        "            try:\n",
        "                print(\"\\nStarting hyperparameter optimization...\")\n",
        "                retrieval_system.optimize_hyperparameters(test_queries, n_trials=9)\n",
        "                print(\"Hyperparameter optimization completed\")\n",
        "            except Exception as e:\n",
        "                print(f\"Hyperparameter optimization failed, using defaults: {str(e)}\")\n",
        "\n",
        "        # Run interactive query loop\n",
        "        retrieval_system.interactive_query_loop()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Fatal error: {str(e)}\")\n",
        "        exit(1)"
      ],
      "metadata": {
        "id": "nuA3Gmp_9k3H"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}